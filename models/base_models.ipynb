{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix ,f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "NUM_FOLDS = 10\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_train_data(path='../data/train_pca_20.csv',training_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values of each feature: \n",
      " pca0    -2.660983e-17\n",
      "pca1    -6.039613e-18\n",
      "pca2    -8.668621e-18\n",
      "pca3     3.272049e-17\n",
      "pca4    -1.250555e-17\n",
      "pca5     2.970069e-17\n",
      "pca6    -3.268497e-17\n",
      "pca7    -4.121148e-18\n",
      "pca8    -1.243450e-17\n",
      "pca9    -8.384404e-18\n",
      "pca10   -6.821210e-18\n",
      "pca11    8.668621e-18\n",
      "pca12   -6.892265e-18\n",
      "pca13   -1.865175e-17\n",
      "pca14    1.008971e-17\n",
      "pca15    4.760636e-18\n",
      "pca16   -1.989520e-17\n",
      "pca17    2.145839e-17\n",
      "pca18   -6.714629e-18\n",
      "pca19   -1.577405e-17\n",
      "dtype: float64\n",
      "Std values of each feature: \n",
      " pca0     1.0\n",
      "pca1     1.0\n",
      "pca2     1.0\n",
      "pca3     1.0\n",
      "pca4     1.0\n",
      "pca5     1.0\n",
      "pca6     1.0\n",
      "pca7     1.0\n",
      "pca8     1.0\n",
      "pca9     1.0\n",
      "pca10    1.0\n",
      "pca11    1.0\n",
      "pca12    1.0\n",
      "pca13    1.0\n",
      "pca14    1.0\n",
      "pca15    1.0\n",
      "pca16    1.0\n",
      "pca17    1.0\n",
      "pca18    1.0\n",
      "pca19    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# standardize the data\n",
    "X = standardize_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test data\n",
    "X_test,y_test = get_test_data(path='../data/train_pca_20.csv',test_size=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Model\n",
    "\n",
    "The DummyClassifier class in scikit-learn provides several strategies for a baseline method, such as predicting the most frequent class label, predicting a random class label, or predicting based on the class distribution of the training set.\n",
    "\n",
    "**Strategy to use to generate predictions:**\n",
    "\n",
    "1. most_frequent:\n",
    "   - The predict method always returns the most frequent class label in the observed y argument passed to fit.\n",
    "   - The predict_proba method returns the matching one-hot encoded vector.\n",
    "2. prior:\n",
    "\n",
    "- The predict method always returns the most frequent class label in the observed y argument passed to fit (like most_frequent).\n",
    "- Predict_proba always returns the empirical class distribution of y also known as the empirical class prior distribution.\n",
    "\n",
    "3. stratified:\n",
    "\n",
    "- The predict_proba method randomly samples one-hot vectors from a multinomial distribution parametrized by the empirical class prior probabilities.\n",
    "- The predict method returns the class label which got probability one in the one-hot vector of predict_proba. Each sampled row of both methods is therefore independent and identically distributed.\n",
    "\n",
    "4. uniform:\n",
    "\n",
    "- Generates predictions uniformly at random from the list of unique classes observed in y, i.e. each class has equal probability.\n",
    "\n",
    "5. constant:\n",
    "\n",
    "- Always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8184699999999999\n",
      "f1_macro:  0.4977632366599372\n",
      "f1_micro:  0.8184699999999999\n"
     ]
    }
   ],
   "source": [
    "# Train ZeroR on the training set\n",
    "zeroR = DummyClassifier(strategy='stratified')\n",
    "zeroR.fit(X, y)\n",
    "\n",
    "cv_results = cross_validate(zeroR, X, y, cv=NUM_FOLDS, scoring=[\n",
    "                            'f1_macro', 'accuracy', 'f1_micro'])\n",
    "\n",
    "print('accuracy: ', cv_results['test_accuracy'].mean())\n",
    "print('f1_macro: ', cv_results['test_f1_macro'].mean())\n",
    "print('f1_micro: ', cv_results['test_f1_micro'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Accuracy: 0.81861\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     89917\n",
      "           1       0.10      0.10      0.10     10083\n",
      "\n",
      "    accuracy                           0.82    100000\n",
      "   macro avg       0.50      0.50      0.50    100000\n",
      "weighted avg       0.82      0.82      0.82    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "zero_r_pred = zeroR.predict(X_test)\n",
    "\n",
    "zero_r_accuracy = accuracy_score(y_test, zero_r_pred)\n",
    "print(\"ZeroR Accuracy:\", zero_r_accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, zero_r_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8998500000000001\n",
      "f1_macro:  0.47364265563458696\n",
      "f1_micro:  0.8998500000000001\n"
     ]
    }
   ],
   "source": [
    "# Train ZeroR on the training set\n",
    "zeroR = DummyClassifier(strategy='most_frequent')\n",
    "zeroR.fit(X, y)\n",
    "\n",
    "cv_results = cross_validate(zeroR, X, y, cv=NUM_FOLDS, scoring=[\n",
    "                            'f1_macro', 'accuracy', 'f1_micro'])\n",
    "\n",
    "print('accuracy: ', cv_results['test_accuracy'].mean())\n",
    "print('f1_macro: ', cv_results['test_f1_macro'].mean())\n",
    "print('f1_micro: ', cv_results['test_f1_micro'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Accuracy: 0.89917\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     89917\n",
      "           1       0.00      0.00      0.00     10083\n",
      "\n",
      "    accuracy                           0.90    100000\n",
      "   macro avg       0.45      0.50      0.47    100000\n",
      "weighted avg       0.81      0.90      0.85    100000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "zero_r_pred = zeroR.predict(X_test)\n",
    "\n",
    "zero_r_accuracy = accuracy_score(y_test, zero_r_pred)\n",
    "print(\"ZeroR Accuracy:\", zero_r_accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, zero_r_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8998500000000001\n",
      "f1_macro:  0.47364265563458696\n",
      "f1_micro:  0.8998500000000001\n"
     ]
    }
   ],
   "source": [
    "# Train ZeroR on the training set\n",
    "zeroR = DummyClassifier(strategy='prior')\n",
    "zeroR.fit(X, y)\n",
    "\n",
    "cv_results = cross_validate(zeroR, X, y, cv=NUM_FOLDS, scoring=[\n",
    "                            'f1_macro', 'accuracy', 'f1_micro'])\n",
    "\n",
    "print('accuracy: ', cv_results['test_accuracy'].mean())\n",
    "print('f1_macro: ', cv_results['test_f1_macro'].mean())\n",
    "print('f1_micro: ', cv_results['test_f1_micro'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Accuracy: 0.89917\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     89917\n",
      "           1       0.00      0.00      0.00     10083\n",
      "\n",
      "    accuracy                           0.90    100000\n",
      "   macro avg       0.45      0.50      0.47    100000\n",
      "weighted avg       0.81      0.90      0.85    100000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "zero_r_pred = zeroR.predict(X_test)\n",
    "\n",
    "zero_r_accuracy = accuracy_score(y_test, zero_r_pred)\n",
    "print(\"ZeroR Accuracy:\", zero_r_accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, zero_r_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5008999999999999\n",
      "f1_macro:  0.4056152067337889\n",
      "f1_micro:  0.5008999999999999\n"
     ]
    }
   ],
   "source": [
    "# Train ZeroR on the training set\n",
    "zeroR = DummyClassifier(strategy='uniform')\n",
    "zeroR.fit(X, y)\n",
    "\n",
    "cv_results = cross_validate(zeroR, X, y, cv=NUM_FOLDS, scoring=[\n",
    "                            'f1_macro', 'accuracy', 'f1_micro'])\n",
    "\n",
    "print('accuracy: ', cv_results['test_accuracy'].mean())\n",
    "print('f1_macro: ', cv_results['test_f1_macro'].mean())\n",
    "print('f1_micro: ', cv_results['test_f1_micro'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Accuracy: 0.49843\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.50      0.64     89917\n",
      "           1       0.10      0.49      0.17     10083\n",
      "\n",
      "    accuracy                           0.50    100000\n",
      "   macro avg       0.50      0.50      0.40    100000\n",
      "weighted avg       0.82      0.50      0.59    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "zero_r_pred = zeroR.predict(X_test)\n",
    "\n",
    "zero_r_accuracy = accuracy_score(y_test, zero_r_pred)\n",
    "print(\"ZeroR Accuracy:\", zero_r_accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, zero_r_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8998500000000001\n",
      "f1_macro:  0.47364265563458696\n",
      "f1_micro:  0.8998500000000001\n"
     ]
    }
   ],
   "source": [
    "# Train ZeroR on the training set\n",
    "zeroR = DummyClassifier(strategy='constant', constant=0)\n",
    "zeroR.fit(X, y)\n",
    "\n",
    "cv_results = cross_validate(zeroR, X, y, cv=NUM_FOLDS, scoring=[\n",
    "                            'f1_macro', 'accuracy', 'f1_micro'])\n",
    "\n",
    "print('accuracy: ', cv_results['test_accuracy'].mean())\n",
    "print('f1_macro: ', cv_results['test_f1_macro'].mean())\n",
    "print('f1_micro: ', cv_results['test_f1_micro'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Accuracy: 0.89917\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     89917\n",
      "           1       0.00      0.00      0.00     10083\n",
      "\n",
      "    accuracy                           0.90    100000\n",
      "   macro avg       0.45      0.50      0.47    100000\n",
      "weighted avg       0.81      0.90      0.85    100000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\peter\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "zero_r_pred = zeroR.predict(X_test)\n",
    "\n",
    "zero_r_accuracy = accuracy_score(y_test, zero_r_pred)\n",
    "print(\"ZeroR Accuracy:\", zero_r_accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, zero_r_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
